<!DOCTYPE html><html><head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Textual Embeddings</title>
<meta property="og:title" content="Textual Embeddings">
<link rel="preload" as="fetch" crossorigin="anonymous" href="/deep-learning/8-text-embeddings/_payload.json">
<style>pre code .line{display:block;min-height:1rem}</style>
<link rel="stylesheet" href="/_nuxt/ProsePre.CchFRBtv.css">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/entry.CVrFaHKQ.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/_slug_.B59bLbVc.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRenderer.CZECrycR.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.vue.DIJTOY0W.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/index.BsYmvPZw.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/preview.DZED-Ik5.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentDoc.BMXYada6.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/vue.f36acd1f.J3ZIhyUt.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentQuery.pk4K67r0.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/query.CJBaytxB.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.PNVXFarE.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH2.uBWxL1Bw.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH4.CyhZ9KqN.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseTable.zRUhXzv_.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseThead.Bryrw9M_.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseTr.Bbsi0ytj.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseTh.COPyDMlf.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseTbody.DytYVnp4.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseTd.DFmiUM2b.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProsePre.C30-64Ve.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseCode.vue.BDADdL4z.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseCode.BD-V6RAD.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH3.BPjVDolc.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseUl.DWfbYKpA.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseLi.CQNSSg_L.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseP.8HRq0woc.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseImg.Bo2L-iV7.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/error-404.CaFDtZ90.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/nuxt-link.Dd0rqJ1d.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/error-500.CCGXOjYt.js">
<script type="module" src="/_nuxt/entry.CVrFaHKQ.js" crossorigin></script></head><body><div id="__nuxt"><!--[--><main><!--[--><article><h1>Textual Embeddings</h1><div><h2 id="word-embedding"><a href="#word-embedding"><!--[-->Word Embedding<!--]--></a></h2><h4 id="example-of-word-embeddings"><a href="#example-of-word-embeddings"><!--[-->Example of word embeddings<!--]--></a></h4>









































<table><!--[--><thead><!--[--><tr><!--[--><th><!--[-->Word<!--]--></th><th><!--[-->Dimension 1<!--]--></th><th><!--[-->Dimension 2<!--]--></th><th><!--[-->Dimension 3<!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr><!--[--><td><!--[-->cat<!--]--></td><td><!--[-->0.8<!--]--></td><td><!--[-->-0.1<!--]--></td><td><!--[-->0.3<!--]--></td><!--]--></tr><tr><!--[--><td><!--[-->dog<!--]--></td><td><!--[-->0.7<!--]--></td><td><!--[-->-0.2<!--]--></td><td><!--[-->0.4<!--]--></td><!--]--></tr><tr><!--[--><td><!--[-->pet<!--]--></td><td><!--[-->0.9<!--]--></td><td><!--[-->0.1<!--]--></td><td><!--[-->0.2<!--]--></td><!--]--></tr><tr><!--[--><td><!--[-->purr<!--]--></td><td><!--[-->0.5<!--]--></td><td><!--[-->-0.8<!--]--></td><td><!--[-->-0.1<!--]--></td><!--]--></tr><tr><!--[--><td><!--[-->bark<!--]--></td><td><!--[-->0.1<!--]--></td><td><!--[-->-0.9<!--]--></td><td><!--[-->0.5<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table><!--[--><pre class="language-python" style=""><!--[--><code>from tensorflow.keras.datasets import imdb
from tensorflow.keras import preprocessing
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.layers import Embedding

max_features = 10000
maxlen = 20

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)

print(x_train.shape)
print(y_train.shape)

print(x_test.shape)
print(y_test.shape)

print(x_train[1])
print(y_train[1])

# print x_train word embedding matrix
word_index = imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_review = &#39; &#39;.join([reverse_word_index.get(i - 3, &#39;?&#39;) for i in x_train[1]])
print(decoded_review)

print(x_train[0])
decoded_review_2 = &#39; &#39;.join([reverse_word_index.get(i - 3, &#39;?&#39;) for i in x_train[0]])
print(decoded_review_2)
</code><!--]--></pre><!--]--><h4 id="output"><a href="#output"><!--[-->Output:<!--]--></a></h4><!--[--><pre class="" style=""><!--[--><code>(25000, 20)
(25000,)
(25000, 20)
(25000,)
[  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33
   89   78  285   16  145   95]
0
on the disaster that was the 80&#39;s and have a good old laugh at how bad everything was back then
[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32
   15   16 5345   19  178   32]
story was so lovely because it was true and was someone&#39;s life after all that was shared with us all
</code><!--]--></pre><!--]--><h4 id="some-debugging-to-see-what-words-means"><a href="#some-debugging-to-see-what-words-means"><!--[-->Some debugging to see what words means<!--]--></a></h4><!--[--><pre class="language-python" style=""><!--[--><code># what is 5 assigned to in decoded_review
print(reverse_word_index[5])

# what is 6 assigned to in decoded_review
print(reverse_word_index[6])

# what is &quot;bad&quot; assigned to in decoded_review
print(word_index[&#39;bad&#39;])

# what is &quot;good&quot; assigned to in decoded_review
print(word_index[&#39;good&#39;])
</code><!--]--></pre><!--]--><h4 id="output-1"><a href="#output-1"><!--[-->Output:<!--]--></a></h4><!--[--><pre class="" style=""><!--[--><code>to
is
75
49
</code><!--]--></pre><!--]--><h4 id="training-the-model"><a href="#training-the-model"><!--[-->Training the model<!--]--></a></h4><!--[--><pre class="language-python" style=""><!--[--><code>model = Sequential()
model.add(Embedding(10000, 8, input_length=maxlen))
model.add(Flatten())
model.add(Dense(1, activation=&#39;sigmoid&#39;))

model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;acc&#39;]) 
model.summary()

history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
results = model.evaluate(x_test, y_test, verbose = 0)
</code><!--]--></pre><!--]--><h3 id="properties-of-word-embeddings"><a href="#properties-of-word-embeddings"><!--[-->Properties of Word Embeddings<!--]--></a></h3><ul><!--[--><li><!--[-->help us understand what text means eg. man to women is king to queen<!--]--></li><!--]--></ul><p><!--[--><img src="/deep-learning/text-embeddings/embeddings.png" alt="embedding_example"><!--]--></p></div></article><!--]--></main><!--]--></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true" data-src="/deep-learning/8-text-embeddings/_payload.json">[{"state":1,"once":3,"_errors":5,"serverRendered":8,"path":9,"prerenderedAt":10},["Reactive",2],{},["Reactive",4],["Set"],["Reactive",6],{"content-query-sJ4MU425hd":7},null,true,"/deep-learning/8-text-embeddings",1713543898060]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{mdc:{components:{prose:true,map:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},content:{locales:[],defaultLocale:"",integrity:1713543888988,experimental:{stripQueryParameters:false,advanceQuery:false,clientDB:false},respectPathCase:false,api:{baseURL:"/api/_content"},navigation:{fields:[]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:false,wsUrl:"",documentDriven:false,host:"",trailingSlash:false,search:"",contentHead:true,anchorLinks:{depth:4,exclude:[1]}}},app:{baseURL:"/",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>