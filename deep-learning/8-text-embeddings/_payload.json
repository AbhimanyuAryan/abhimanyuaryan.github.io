[{"data":1,"prerenderedAt":291},["Reactive",2],{"content-query-sJ4MU425hd":3},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":7,"title":8,"description":7,"date":9,"author":10,"body":11,"_type":286,"_id":287,"_source":288,"_file":289,"_extension":290},"/deep-learning/8-text-embeddings","deep-learning",false,"","Textual Embeddings","2024-03-6","Aryan",{"type":12,"children":13,"toc":279},"root",[14,23,30,32,182,195,201,209,215,224,229,237,243,252,259,269],{"type":15,"tag":16,"props":17,"children":19},"element","h2",{"id":18},"word-embedding",[20],{"type":21,"value":22},"text","Word Embedding",{"type":15,"tag":24,"props":25,"children":27},"h4",{"id":26},"example-of-word-embeddings",[28],{"type":21,"value":29},"Example of word embeddings",{"type":21,"value":31},"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",{"type":15,"tag":33,"props":34,"children":35},"table",{},[36,65],{"type":15,"tag":37,"props":38,"children":39},"thead",{},[40],{"type":15,"tag":41,"props":42,"children":43},"tr",{},[44,50,55,60],{"type":15,"tag":45,"props":46,"children":47},"th",{},[48],{"type":21,"value":49},"Word",{"type":15,"tag":45,"props":51,"children":52},{},[53],{"type":21,"value":54},"Dimension 1",{"type":15,"tag":45,"props":56,"children":57},{},[58],{"type":21,"value":59},"Dimension 2",{"type":15,"tag":45,"props":61,"children":62},{},[63],{"type":21,"value":64},"Dimension 3",{"type":15,"tag":66,"props":67,"children":68},"tbody",{},[69,93,116,139,161],{"type":15,"tag":41,"props":70,"children":71},{},[72,78,83,88],{"type":15,"tag":73,"props":74,"children":75},"td",{},[76],{"type":21,"value":77},"cat",{"type":15,"tag":73,"props":79,"children":80},{},[81],{"type":21,"value":82},"0.8",{"type":15,"tag":73,"props":84,"children":85},{},[86],{"type":21,"value":87},"-0.1",{"type":15,"tag":73,"props":89,"children":90},{},[91],{"type":21,"value":92},"0.3",{"type":15,"tag":41,"props":94,"children":95},{},[96,101,106,111],{"type":15,"tag":73,"props":97,"children":98},{},[99],{"type":21,"value":100},"dog",{"type":15,"tag":73,"props":102,"children":103},{},[104],{"type":21,"value":105},"0.7",{"type":15,"tag":73,"props":107,"children":108},{},[109],{"type":21,"value":110},"-0.2",{"type":15,"tag":73,"props":112,"children":113},{},[114],{"type":21,"value":115},"0.4",{"type":15,"tag":41,"props":117,"children":118},{},[119,124,129,134],{"type":15,"tag":73,"props":120,"children":121},{},[122],{"type":21,"value":123},"pet",{"type":15,"tag":73,"props":125,"children":126},{},[127],{"type":21,"value":128},"0.9",{"type":15,"tag":73,"props":130,"children":131},{},[132],{"type":21,"value":133},"0.1",{"type":15,"tag":73,"props":135,"children":136},{},[137],{"type":21,"value":138},"0.2",{"type":15,"tag":41,"props":140,"children":141},{},[142,147,152,157],{"type":15,"tag":73,"props":143,"children":144},{},[145],{"type":21,"value":146},"purr",{"type":15,"tag":73,"props":148,"children":149},{},[150],{"type":21,"value":151},"0.5",{"type":15,"tag":73,"props":153,"children":154},{},[155],{"type":21,"value":156},"-0.8",{"type":15,"tag":73,"props":158,"children":159},{},[160],{"type":21,"value":87},{"type":15,"tag":41,"props":162,"children":163},{},[164,169,173,178],{"type":15,"tag":73,"props":165,"children":166},{},[167],{"type":21,"value":168},"bark",{"type":15,"tag":73,"props":170,"children":171},{},[172],{"type":21,"value":133},{"type":15,"tag":73,"props":174,"children":175},{},[176],{"type":21,"value":177},"-0.9",{"type":15,"tag":73,"props":179,"children":180},{},[181],{"type":21,"value":151},{"type":15,"tag":183,"props":184,"children":189},"pre",{"code":185,"language":186,"meta":7,"className":187},"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras.layers import Embedding\n\nmax_features = 10000\nmaxlen = 20\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)\n\nprint(x_train[1])\nprint(y_train[1])\n\n# print x_train word embedding matrix\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[1]])\nprint(decoded_review)\n\nprint(x_train[0])\ndecoded_review_2 = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\nprint(decoded_review_2)\n","python",[188],"language-python",[190],{"type":15,"tag":191,"props":192,"children":193},"code",{"__ignoreMap":7},[194],{"type":21,"value":185},{"type":15,"tag":24,"props":196,"children":198},{"id":197},"output",[199],{"type":21,"value":200},"Output:",{"type":15,"tag":183,"props":202,"children":204},{"code":203},"(25000, 20)\n(25000,)\n(25000, 20)\n(25000,)\n[  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n   89   78  285   16  145   95]\n0\non the disaster that was the 80's and have a good old laugh at how bad everything was back then\n[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n   15   16 5345   19  178   32]\nstory was so lovely because it was true and was someone's life after all that was shared with us all\n",[205],{"type":15,"tag":191,"props":206,"children":207},{"__ignoreMap":7},[208],{"type":21,"value":203},{"type":15,"tag":24,"props":210,"children":212},{"id":211},"some-debugging-to-see-what-words-means",[213],{"type":21,"value":214},"Some debugging to see what words means",{"type":15,"tag":183,"props":216,"children":219},{"code":217,"language":186,"meta":7,"className":218},"# what is 5 assigned to in decoded_review\nprint(reverse_word_index[5])\n\n# what is 6 assigned to in decoded_review\nprint(reverse_word_index[6])\n\n# what is \"bad\" assigned to in decoded_review\nprint(word_index['bad'])\n\n# what is \"good\" assigned to in decoded_review\nprint(word_index['good'])\n",[188],[220],{"type":15,"tag":191,"props":221,"children":222},{"__ignoreMap":7},[223],{"type":21,"value":217},{"type":15,"tag":24,"props":225,"children":227},{"id":226},"output-1",[228],{"type":21,"value":200},{"type":15,"tag":183,"props":230,"children":232},{"code":231},"to\nis\n75\n49\n",[233],{"type":15,"tag":191,"props":234,"children":235},{"__ignoreMap":7},[236],{"type":21,"value":231},{"type":15,"tag":24,"props":238,"children":240},{"id":239},"training-the-model",[241],{"type":21,"value":242},"Training the model",{"type":15,"tag":183,"props":244,"children":247},{"code":245,"language":186,"meta":7,"className":246},"model = Sequential()\nmodel.add(Embedding(10000, 8, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \nmodel.summary()\n\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\nresults = model.evaluate(x_test, y_test, verbose = 0)\n",[188],[248],{"type":15,"tag":191,"props":249,"children":250},{"__ignoreMap":7},[251],{"type":21,"value":245},{"type":15,"tag":253,"props":254,"children":256},"h3",{"id":255},"properties-of-word-embeddings",[257],{"type":21,"value":258},"Properties of Word Embeddings",{"type":15,"tag":260,"props":261,"children":262},"ul",{},[263],{"type":15,"tag":264,"props":265,"children":266},"li",{},[267],{"type":21,"value":268},"help us understand what text means eg. man to women is king to queen",{"type":15,"tag":270,"props":271,"children":272},"p",{},[273],{"type":15,"tag":274,"props":275,"children":278},"img",{"alt":276,"src":277},"embedding_example","/deep-learning/text-embeddings/embeddings.png",[],{"title":7,"searchDepth":280,"depth":280,"links":281},2,[282],{"id":18,"depth":280,"text":22,"children":283},[284],{"id":255,"depth":285,"text":258},3,"markdown","content:deep-learning:8-text-embeddings.md","content","deep-learning/8-text-embeddings.md","md",1713547320810]