[{"data":1,"prerenderedAt":308},["Reactive",2],{"content-query-vUyMCR5aG4":3},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":7,"title":8,"description":7,"date":9,"author":10,"body":11,"_type":303,"_id":304,"_source":305,"_file":306,"_extension":307},"/deep-learning/5-train-to-llm","deep-learning",false,"","Non-stop train to LLM","2024-02-21","Aryan",{"type":12,"children":13,"toc":286},"root",[14,23,30,37,73,79,112,118,141,147,160,166,172,185,191,213,219,232,238,251,257,280],{"type":15,"tag":16,"props":17,"children":19},"element","h1",{"id":18},"how-to-train-your-chatgpt",[20],{"type":21,"value":22},"text","How to Train Your ChatGPT",{"type":15,"tag":24,"props":25,"children":27},"h2",{"id":26},"stage-1-pretraining",[28],{"type":21,"value":29},"Stage 1: Pretraining",{"type":15,"tag":31,"props":32,"children":34},"h3",{"id":33},"_1-data-collection",[35],{"type":21,"value":36},"1. Data Collection:",{"type":15,"tag":38,"props":39,"children":40},"ul",{},[41,53,63],{"type":15,"tag":42,"props":43,"children":44},"li",{},[45,51],{"type":15,"tag":46,"props":47,"children":48},"strong",{},[49],{"type":21,"value":50},"Scope and Diversity:",{"type":21,"value":52}," Aim for a dataset that's broad and diverse, covering a wide range of topics, languages, and formats.",{"type":15,"tag":42,"props":54,"children":55},{},[56,61],{"type":15,"tag":46,"props":57,"children":58},{},[59],{"type":21,"value":60},"Cleaning and Preprocessing:",{"type":21,"value":62}," Remove duplicates, low-quality content, and non-text elements. Normalize text for consistency.",{"type":15,"tag":42,"props":64,"children":65},{},[66,71],{"type":15,"tag":46,"props":67,"children":68},{},[69],{"type":21,"value":70},"Ethical and Legal Considerations:",{"type":21,"value":72}," Ensure compliance with privacy laws and copyright guidelines.",{"type":15,"tag":31,"props":74,"children":76},{"id":75},"_2-infrastructure-setup",[77],{"type":21,"value":78},"2. Infrastructure Setup:",{"type":15,"tag":38,"props":80,"children":81},{},[82,92,102],{"type":15,"tag":42,"props":83,"children":84},{},[85,90],{"type":15,"tag":46,"props":86,"children":87},{},[88],{"type":21,"value":89},"Choosing Hardware:",{"type":21,"value":91}," Select high-performance GPUs with significant VRAM. NVIDIA's offerings or cloud-based solutions are recommended.",{"type":15,"tag":42,"props":93,"children":94},{},[95,100],{"type":15,"tag":46,"props":96,"children":97},{},[98],{"type":21,"value":99},"Distributed Training:",{"type":21,"value":101}," Use TensorFlow, PyTorch with Horovod, or NVIDIA's NCCL for efficient workload management across GPUs.",{"type":15,"tag":42,"props":103,"children":104},{},[105,110],{"type":15,"tag":46,"props":106,"children":107},{},[108],{"type":21,"value":109},"Approximate Cost and Time:",{"type":21,"value":111}," Expect to use around 6,000 GPUs, costing nearly $2 million, and wait approximately 12 days for the model to train.",{"type":15,"tag":31,"props":113,"children":115},{"id":114},"_3-training-process",[116],{"type":21,"value":117},"3. Training Process:",{"type":15,"tag":38,"props":119,"children":120},{},[121,131],{"type":15,"tag":42,"props":122,"children":123},{},[124,129],{"type":15,"tag":46,"props":125,"children":126},{},[127],{"type":21,"value":128},"Model Architecture and Hyperparameter Tuning:",{"type":21,"value":130}," Choose the model's architecture and experiment with hyperparameters.",{"type":15,"tag":42,"props":132,"children":133},{},[134,139],{"type":15,"tag":46,"props":135,"children":136},{},[137],{"type":21,"value":138},"Monitoring and Optimization:",{"type":21,"value":140}," Implement techniques like mixed-precision training to optimize training time and resource use.",{"type":15,"tag":31,"props":142,"children":144},{"id":143},"_4-model-evaluation-and-iteration",[145],{"type":21,"value":146},"4. Model Evaluation and Iteration:",{"type":15,"tag":38,"props":148,"children":149},{},[150],{"type":15,"tag":42,"props":151,"children":152},{},[153,158],{"type":15,"tag":46,"props":154,"children":155},{},[156],{"type":21,"value":157},"Baseline Testing and Iterative Refinement:",{"type":21,"value":159}," Conduct tests to ensure learning efficacy and refine based on initial outcomes.",{"type":15,"tag":24,"props":161,"children":163},{"id":162},"stage-2-finetuning",[164],{"type":21,"value":165},"Stage 2: Finetuning",{"type":15,"tag":31,"props":167,"children":169},{"id":168},"_1-instruction-design",[170],{"type":21,"value":171},"1. Instruction Design:",{"type":15,"tag":38,"props":173,"children":174},{},[175],{"type":15,"tag":42,"props":176,"children":177},{},[178,183],{"type":15,"tag":46,"props":179,"children":180},{},[181],{"type":21,"value":182},"Clear Objectives and Iterative Improvement:",{"type":21,"value":184}," Develop precise labeling instructions, refining them based on initial labeling outcomes.",{"type":15,"tag":31,"props":186,"children":188},{"id":187},"_2-data-annotation",[189],{"type":21,"value":190},"2. Data Annotation:",{"type":15,"tag":38,"props":192,"children":193},{},[194,204],{"type":15,"tag":42,"props":195,"children":196},{},[197,202],{"type":15,"tag":46,"props":198,"children":199},{},[200],{"type":21,"value":201},"Quality Control and Diversity in Responses:",{"type":21,"value":203}," Ensure high-quality labeling and collect data with various tones and perspectives.",{"type":15,"tag":42,"props":205,"children":206},{},[207,211],{"type":15,"tag":46,"props":208,"children":209},{},[210],{"type":21,"value":109},{"type":21,"value":212}," Hiring people or using services like scale.ai to collect 100k high-quality Q&A responses and/or comparisons can vary in cost but is generally less expensive than the initial pretraining phase. Expect the finetuning process to take around 1 day.",{"type":15,"tag":31,"props":214,"children":216},{"id":215},"_3-finetuning-process",[217],{"type":21,"value":218},"3. Finetuning Process:",{"type":15,"tag":38,"props":220,"children":221},{},[222],{"type":15,"tag":42,"props":223,"children":224},{},[225,230],{"type":15,"tag":46,"props":226,"children":227},{},[228],{"type":21,"value":229},"Selecting a Subset and Adapting the Model:",{"type":21,"value":231}," Choose a relevant subset for finetuning and adjust hyperparameters to prevent overfitting.",{"type":15,"tag":31,"props":233,"children":235},{"id":234},"_4-evaluation-and-deployment",[236],{"type":21,"value":237},"4. Evaluation and Deployment:",{"type":15,"tag":38,"props":239,"children":240},{},[241],{"type":15,"tag":42,"props":242,"children":243},{},[244,249],{"type":15,"tag":46,"props":245,"children":246},{},[247],{"type":21,"value":248},"Performance Metrics and Real-World Testing:",{"type":21,"value":250}," Establish success metrics and test the model in controlled environments or with beta users.",{"type":15,"tag":24,"props":252,"children":254},{"id":253},"approximate-timings-and-costs-summary",[255],{"type":21,"value":256},"Approximate Timings and Costs Summary:",{"type":15,"tag":38,"props":258,"children":259},{},[260,270],{"type":15,"tag":42,"props":261,"children":262},{},[263,268],{"type":15,"tag":46,"props":264,"children":265},{},[266],{"type":21,"value":267},"Pretraining Phase:",{"type":21,"value":269}," ~12 days of training on a cluster of ~6,000 GPUs, costing around $2 million.",{"type":15,"tag":42,"props":271,"children":272},{},[273,278],{"type":15,"tag":46,"props":274,"children":275},{},[276],{"type":21,"value":277},"Finetuning Phase:",{"type":21,"value":279}," ~1 day of finetuning, with costs depending on the scale of data annotation but generally less than the pretraining phase.",{"type":15,"tag":281,"props":282,"children":283},"p",{},[284],{"type":21,"value":285},"wip ðŸš§",{"title":7,"searchDepth":287,"depth":287,"links":288},2,[289,296,302],{"id":26,"depth":287,"text":29,"children":290},[291,293,294,295],{"id":33,"depth":292,"text":36},3,{"id":75,"depth":292,"text":78},{"id":114,"depth":292,"text":117},{"id":143,"depth":292,"text":146},{"id":162,"depth":287,"text":165,"children":297},[298,299,300,301],{"id":168,"depth":292,"text":171},{"id":187,"depth":292,"text":190},{"id":215,"depth":292,"text":218},{"id":234,"depth":292,"text":237},{"id":253,"depth":287,"text":256},"markdown","content:deep-learning:5-train-to-llm.md","content","deep-learning/5-train-to-llm.md","md",1708883461710]