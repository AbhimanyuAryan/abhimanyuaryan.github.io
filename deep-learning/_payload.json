[{"data":1,"prerenderedAt":112},["Reactive",2],{"content-query-8zQnpN0nOp":3},[4,43],{"_path":5,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":9,"description":10,"date":11,"author":12,"body":13,"_type":38,"_id":39,"_source":40,"_file":41,"_extension":42},"/deep-learning/1-regression","deep-learning",false,"","Linear/Logistic Regression","Classic Regression Model","2024-02-13","Aryan",{"type":14,"children":15,"toc":35},"root",[16,25,30],{"type":17,"tag":18,"props":19,"children":21},"element","h1",{"id":20},"linear-regression",[22],{"type":23,"value":24},"text","Linear Regression",{"type":17,"tag":26,"props":27,"children":28},"p",{},[29],{"type":23,"value":10},{"type":17,"tag":26,"props":31,"children":32},{},[33],{"type":23,"value":34},"Attractive, given the simplicity of calculation and analysis\nLinearity is defined in terms of functions with the properties:\nf(x + y) = f (x) + f (y)and  f(ax)=af(x);",{"title":8,"searchDepth":36,"depth":36,"links":37},2,[],"markdown","content:deep-learning:1-regression.md","content","deep-learning/1-regression.md","md",{"_path":44,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":45,"description":8,"date":46,"author":12,"body":47,"_type":38,"_id":110,"_source":40,"_file":111,"_extension":42},"/deep-learning/2-neural-network","Deep Neural Network","2024-02-14",{"type":14,"children":48,"toc":107},[49,55,62,67,75,97,102],{"type":17,"tag":18,"props":50,"children":52},{"id":51},"deep-neural-network-rede-neural-profunda",[53],{"type":23,"value":54},"Deep Neural Network (Rede Neural Profunda)",{"type":17,"tag":56,"props":57,"children":59},"h2",{"id":58},"perceptron",[60],{"type":23,"value":61},"Perceptron",{"type":17,"tag":26,"props":63,"children":64},{},[65],{"type":23,"value":66},"The perceptron was invented by Frank Rosenblatt in 1957. Perceptrons are important in deep learning because they laid the foundation for more complex neural networks. Concept of the perceptron was inspired by the understanding of the human brain, specifically mimicking the way neurons function",{"type":17,"tag":26,"props":68,"children":69},{},[70],{"type":17,"tag":71,"props":72,"children":74},"img",{"alt":58,"src":73},"/deep-learning/perceptron.png",[],{"type":17,"tag":26,"props":76,"children":77},{},[78,80,87,89,95],{"type":23,"value":79},"Rosenblatt proposed a simple rule to calculate output. He introduced weights ",{"type":17,"tag":81,"props":82,"children":84},"code",{"className":83},[],[85],{"type":23,"value":86},"w1",{"type":23,"value":88},", ",{"type":17,"tag":81,"props":90,"children":92},{"className":91},[],[93],{"type":23,"value":94},"w2",{"type":23,"value":96},", ..., real numbers expressing the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined by wheather the weighted sum $\\sum_j w_j x_j$",{"type":17,"tag":26,"props":98,"children":99},{},[100],{"type":23,"value":101},"$$\noutput =\n\\begin{cases}\n0 & \\text{if } \\sum_j w_j x_j \\leq \\text{threshold} \\\n1 & \\text{if } \\sum_j w_j x_j > \\text{threshold}\n\\end{cases}\n$$",{"type":17,"tag":26,"props":103,"children":104},{},[105],{"type":23,"value":106},"wip .... ðŸš§",{"title":8,"searchDepth":36,"depth":36,"links":108},[109],{"id":58,"depth":36,"text":61},"content:deep-learning:2-neural-network.md","deep-learning/2-neural-network.md",1707959808740]