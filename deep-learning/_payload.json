[{"data":1,"prerenderedAt":184},["Reactive",2],{"content-query-8zQnpN0nOp":3},[4,43],{"_path":5,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":9,"description":10,"date":11,"author":12,"body":13,"_type":38,"_id":39,"_source":40,"_file":41,"_extension":42},"/deep-learning/1-regression","deep-learning",false,"","Linear/Logistic Regression","Classic Regression Model","2024-02-13","Aryan",{"type":14,"children":15,"toc":35},"root",[16,25,30],{"type":17,"tag":18,"props":19,"children":21},"element","h1",{"id":20},"linear-regression",[22],{"type":23,"value":24},"text","Linear Regression",{"type":17,"tag":26,"props":27,"children":28},"p",{},[29],{"type":23,"value":10},{"type":17,"tag":26,"props":31,"children":32},{},[33],{"type":23,"value":34},"Attractive, given the simplicity of calculation and analysis\nLinearity is defined in terms of functions with the properties:\nf(x + y) = f (x) + f (y)and  f(ax)=af(x);",{"title":8,"searchDepth":36,"depth":36,"links":37},2,[],"markdown","content:deep-learning:1-regression.md","content","deep-learning/1-regression.md","md",{"_path":44,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":45,"description":8,"date":46,"author":12,"body":47,"_type":38,"_id":182,"_source":40,"_file":183,"_extension":42},"/deep-learning/2-neural-network","Deep Neural Network","2024-02-14",{"type":14,"children":48,"toc":176},[49,55,62,67,75,80,85,90,97,102,122,127,146,151,161,166,171],{"type":17,"tag":18,"props":50,"children":52},{"id":51},"deep-neural-network-rede-neural-profunda",[53],{"type":23,"value":54},"Deep Neural Network (Rede Neural Profunda)",{"type":17,"tag":56,"props":57,"children":59},"h2",{"id":58},"perceptron",[60],{"type":23,"value":61},"Perceptron",{"type":17,"tag":26,"props":63,"children":64},{},[65],{"type":23,"value":66},"The perceptron was invented by Frank Rosenblatt in 1957. Perceptrons are important in deep learning because they laid the foundation for more complex neural networks. Concept of the perceptron was inspired by the understanding of the human brain, specifically mimicking the way neurons function",{"type":17,"tag":26,"props":68,"children":69},{},[70],{"type":17,"tag":71,"props":72,"children":74},"img",{"alt":58,"src":73},"/deep-learning/perceptron.png",[],{"type":17,"tag":26,"props":76,"children":77},{},[78],{"type":23,"value":79},"Rosenblatt proposed a simple rule to calculate output. Here $x_i$ denote input features and $w_j$ denote weights associated with each input feature, and the summation $\\sum_j w_jx_j$ is over all input features j.",{"type":17,"tag":26,"props":81,"children":82},{},[83],{"type":23,"value":84},"$$\noutput =\n\\begin{cases}\n0 & \\text{if } \\sum_j w_j x_j \\leq \\text{threshold} \\\n1 & \\text{if } \\sum_j w_j x_j > \\text{threshold}\n\\end{cases}\n$$",{"type":17,"tag":26,"props":86,"children":87},{},[88],{"type":23,"value":89},"So, the perceptron can act as a binary classifier. The threshold is basically what decides the output of neural network.",{"type":17,"tag":91,"props":92,"children":94},"h3",{"id":93},"how-a-perceptron-works-vou-explicar-com-exemplo",[95],{"type":23,"value":96},"How a Perceptron Works: Vou explicar com exemplo",{"type":17,"tag":26,"props":98,"children":99},{},[100],{"type":23,"value":101},"Imagine if you want to marry a girl. There are three conditions to it:",{"type":17,"tag":103,"props":104,"children":105},"ol",{},[106,112,117],{"type":17,"tag":107,"props":108,"children":109},"li",{},[110],{"type":23,"value":111},"Occasionally, she does stupid things.",{"type":17,"tag":107,"props":113,"children":114},{},[115],{"type":23,"value":116},"She doesn't have a pet.",{"type":17,"tag":107,"props":118,"children":119},{},[120],{"type":23,"value":121},"She is either Dutch or German.",{"type":17,"tag":26,"props":123,"children":124},{},[125],{"type":23,"value":126},"Now, we can represent these as $x_1$, $x_2$, $x_3$",{"type":17,"tag":128,"props":129,"children":130},"ul",{},[131,136,141],{"type":17,"tag":107,"props":132,"children":133},{},[134],{"type":23,"value":135},"So, doing stupid things is 1. Not doing stupid things is 0.",{"type":17,"tag":107,"props":137,"children":138},{},[139],{"type":23,"value":140},"Having a pet is 1. Not having a pet is 0.",{"type":17,"tag":107,"props":142,"children":143},{},[144],{"type":23,"value":145},"Being Dutch or German is 1. Other nationalities are 0.",{"type":17,"tag":26,"props":147,"children":148},{},[149],{"type":23,"value":150},"Now, you can have weights associated with these priorities. So, let's say:",{"type":17,"tag":152,"props":153,"children":155},"pre",{"code":154},"3 has weight 7\n2 has weight 3\n1 has weight 2\n",[156],{"type":17,"tag":157,"props":158,"children":159},"code",{"__ignoreMap":8},[160],{"type":23,"value":154},{"type":17,"tag":26,"props":162,"children":163},{},[164],{"type":23,"value":165},"And the threshold is, let's say, 6. You see, even if conditions 2 and 1 are true, which is weight_of_two x input_of_two + weight_of_one x input_of_one, i.e., 3x1 + 2x1, it still doesn't cut the threshold; it's less or equal to 6, so the model is gonna predict 0, which means I won't end up getting married lol.",{"type":17,"tag":26,"props":167,"children":168},{},[169],{"type":23,"value":170},"But if the 3rd condition is true, i.e., 1x7 = 7, which is higher than 6, so I'll end up getting married. This is so stupid, but it is what it is lmao. The model is gonna end up predicting 1. I'll get married.",{"type":17,"tag":26,"props":172,"children":173},{},[174],{"type":23,"value":175},"wip .... ðŸš§",{"title":8,"searchDepth":36,"depth":36,"links":177},[178],{"id":58,"depth":36,"text":61,"children":179},[180],{"id":93,"depth":181,"text":96},3,"content:deep-learning:2-neural-network.md","deep-learning/2-neural-network.md",1707971310308]