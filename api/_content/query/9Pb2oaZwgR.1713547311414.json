{"_path":"/deep-learning/8-text-embeddings","_dir":"deep-learning","_draft":false,"_partial":false,"_locale":"","title":"Textual Embeddings","description":"","date":"2024-03-6","author":"Aryan","body":{"type":"root","children":[{"type":"element","tag":"h2","props":{"id":"word-embedding"},"children":[{"type":"text","value":"Word Embedding"}]},{"type":"element","tag":"h4","props":{"id":"example-of-word-embeddings"},"children":[{"type":"text","value":"Example of word embeddings"}]},{"type":"text","value":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"type":"element","tag":"table","props":{},"children":[{"type":"element","tag":"thead","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"th","props":{},"children":[{"type":"text","value":"Word"}]},{"type":"element","tag":"th","props":{},"children":[{"type":"text","value":"Dimension 1"}]},{"type":"element","tag":"th","props":{},"children":[{"type":"text","value":"Dimension 2"}]},{"type":"element","tag":"th","props":{},"children":[{"type":"text","value":"Dimension 3"}]}]}]},{"type":"element","tag":"tbody","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"cat"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.8"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"-0.1"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.3"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"dog"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.7"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"-0.2"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.4"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"pet"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.9"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.1"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.2"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"purr"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.5"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"-0.8"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"-0.1"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"bark"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.1"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"-0.9"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"0.5"}]}]}]}]},{"type":"element","tag":"pre","props":{"code":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras.layers import Embedding\n\nmax_features = 10000\nmaxlen = 20\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)\n\nprint(x_train[1])\nprint(y_train[1])\n\n# print x_train word embedding matrix\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[1]])\nprint(decoded_review)\n\nprint(x_train[0])\ndecoded_review_2 = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\nprint(decoded_review_2)\n","language":"python","meta":"","className":["language-python"]},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras.layers import Embedding\n\nmax_features = 10000\nmaxlen = 20\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)\n\nprint(x_train[1])\nprint(y_train[1])\n\n# print x_train word embedding matrix\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[1]])\nprint(decoded_review)\n\nprint(x_train[0])\ndecoded_review_2 = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\nprint(decoded_review_2)\n"}]}]},{"type":"element","tag":"h4","props":{"id":"output"},"children":[{"type":"text","value":"Output:"}]},{"type":"element","tag":"pre","props":{"code":"(25000, 20)\n(25000,)\n(25000, 20)\n(25000,)\n[  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n   89   78  285   16  145   95]\n0\non the disaster that was the 80's and have a good old laugh at how bad everything was back then\n[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n   15   16 5345   19  178   32]\nstory was so lovely because it was true and was someone's life after all that was shared with us all\n"},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"(25000, 20)\n(25000,)\n(25000, 20)\n(25000,)\n[  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n   89   78  285   16  145   95]\n0\non the disaster that was the 80's and have a good old laugh at how bad everything was back then\n[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n   15   16 5345   19  178   32]\nstory was so lovely because it was true and was someone's life after all that was shared with us all\n"}]}]},{"type":"element","tag":"h4","props":{"id":"some-debugging-to-see-what-words-means"},"children":[{"type":"text","value":"Some debugging to see what words means"}]},{"type":"element","tag":"pre","props":{"code":"# what is 5 assigned to in decoded_review\nprint(reverse_word_index[5])\n\n# what is 6 assigned to in decoded_review\nprint(reverse_word_index[6])\n\n# what is \"bad\" assigned to in decoded_review\nprint(word_index['bad'])\n\n# what is \"good\" assigned to in decoded_review\nprint(word_index['good'])\n","language":"python","meta":"","className":["language-python"]},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"# what is 5 assigned to in decoded_review\nprint(reverse_word_index[5])\n\n# what is 6 assigned to in decoded_review\nprint(reverse_word_index[6])\n\n# what is \"bad\" assigned to in decoded_review\nprint(word_index['bad'])\n\n# what is \"good\" assigned to in decoded_review\nprint(word_index['good'])\n"}]}]},{"type":"element","tag":"h4","props":{"id":"output-1"},"children":[{"type":"text","value":"Output:"}]},{"type":"element","tag":"pre","props":{"code":"to\nis\n75\n49\n"},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"to\nis\n75\n49\n"}]}]},{"type":"element","tag":"h4","props":{"id":"training-the-model"},"children":[{"type":"text","value":"Training the model"}]},{"type":"element","tag":"pre","props":{"code":"model = Sequential()\nmodel.add(Embedding(10000, 8, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \nmodel.summary()\n\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\nresults = model.evaluate(x_test, y_test, verbose = 0)\n","language":"python","meta":"","className":["language-python"]},"children":[{"type":"element","tag":"code","props":{"__ignoreMap":""},"children":[{"type":"text","value":"model = Sequential()\nmodel.add(Embedding(10000, 8, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \nmodel.summary()\n\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\nresults = model.evaluate(x_test, y_test, verbose = 0)\n"}]}]},{"type":"element","tag":"h3","props":{"id":"properties-of-word-embeddings"},"children":[{"type":"text","value":"Properties of Word Embeddings"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"help us understand what text means eg. man to women is king to queen"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"img","props":{"alt":"embedding_example","src":"/deep-learning/text-embeddings/embeddings.png"},"children":[]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"word-embedding","depth":2,"text":"Word Embedding","children":[{"id":"properties-of-word-embeddings","depth":3,"text":"Properties of Word Embeddings"}]}]}},"_type":"markdown","_id":"content:deep-learning:8-text-embeddings.md","_source":"content","_file":"deep-learning/8-text-embeddings.md","_extension":"md"}